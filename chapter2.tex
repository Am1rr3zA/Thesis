\chapter{Detector Combination} \label{chapter2}




 \section{Boolean Combination of Detectors in the ROC Space}
\label{sec:boolean-combination}

This section provides information about ROC analysis and summarizes the Boolean combination in the ROC space \cite{Khreich2010-ICPR}.
A crisp detector outputs a decision or a class label (e.g., normal or anomaly) while a soft detector such as HMM assigns scores to the input samples, which can be converted to a crisp detector by setting a decision threshold on the scores.
Given the responses of a crisp detector on a validation set, the true positive rate ($tpr$) is the proportion of positives correctly classified over the total number of positive samples.
The false positive rate ($fpr$) is the proportion of negatives incorrectly classified over the total number of negative samples.
The positive (or target) class is typically the class of interest, which is the anomalous class for an ADS.

A ROC curve is a plot of $tpr$ against $fpr$ \cite{Fawcett2006}.
A crisp detector produces a single data point in the ROC space, while a soft detector produces a ROC curve by varying the decision thresholds.
In practice, an empirical ROC plot is obtained by connecting the observed ($tpr,fpr$) pairs of a soft detector at each decision threshold.
A point $a$ is \textit{superior} to another point $b$ in the ROC space, if $fpr(a) \leq fpr(b)$ and $tpr(a) \geq tpr(b)$.
The ROC convex hull (ROCCH) is therefore the outer envelope connecting superior points in the ROC space.
A ROC curve allows to visualize the performance of detectors and to select optimal operational points, without committing to a single decision threshold or to fixed error costs.
The area under the ROC curve (AUC), or under the ROCCH, provides a general measure for evaluation and selection of detectors \cite{Fawcett2006}.

The IBC is a general decision-level combination technique that attempts to select the decision thresholds (from each input detector) and the Boolean functions that maximize the overall ROCCH of the combined ensemble \cite{Khreich2010-ICPR}.
The core of IBC (only for the first iteration) is described in Algorithm~\ref{IBC1}.

%%%%%% Algorithms

\input{algos/IBC1.tex}
\input{algos/PBC.tex}

The IBC applies each Boolean function to combine the responses corresponding to each decision threshold from the first detector to those from the second detector.
Fused responses are then mapped to vertices in the ROC space, and their ROC convex hull (ROCCH) is computed.
Vertices that are superior to the ROCCH of original detectors are then selected.
The set ($\mathcal{S}$) of decision thresholds from each detector and Boolean functions corresponding to these vertices is stored, and the ROCCH is updated to include emerging vertices.
The responses corresponding to each decision threshold from the third detector are then combined with the responses of each emerging vertex, and so on, until the last detector in the pool is combined.
The BC technique yields a final ROCCH for visualization and selection of operating points, and the set of selected thresholds and Boolean functions, $\mathcal{S}$, for each vertex on the composite ROCCH to be applied during operations.
Although not shown in Algorithm~\ref{IBC1}, the original IBC algorithm can iterate by re-combining the resulting combination (of all detectors) on the ROCCH with each of the original detectors (sequentially) until the ROCCH stops improving \cite{Khreich2010-ICPR}.

However, as stated previously, combining all available detectors without pruning may be inefficient due to the redundancy in their outputs.
The sequential combinations of soft detectors, illustrated in the loop in line 16 of Algorithm~\ref{IBC1}, produces a sequence of combination rules that grows linearly with $K$ (and the number of iterations), which becomes difficult to track, analyse and understand when the value of $K$ becomes large.
In addition, the IBC algorithm is sensitive to the order in which the detectors are input for combinations, which increases the effort required to find best subset for operations.
% or to control the number of combined detectors.

 \section{Synthetic Data Set}
\label{sec:synthetic}

In order to get a better insight about the reason of the combination and selecting the proper detectors among millions of trained detectors,in this section, we provide examples of training, pruning and combining detectors on a synthetic data set. 

For this purpose, we chose three known synthetic data sets~\cite{Duin2000}. 
\begin{enumerate}
  \item Lithuanian
  \item Banana (Moon)
  \item Circle
\end{enumerate}  
Unlike real data sets, it is possible to plot these data sets and show the trained models generated by them. This makes it easier for us, to see what happens when we prune some unnecessary trained models and keeping only those that can increase the detection accuracy.

Figure~\ref{Figure::lithuanian}, \ref{Figure::banana} and \ref{Figure::circle} show the Lithunian, Banana and Circle data set and Figure~\ref{Figure::lithuanian_all}, \ref{Figure::banana_all} and \ref{Figure::circle_all} show the models trained on them.

\begin{figure}[]
\centering
\includegraphics[scale=0.6]{figs/dataset_Lithuanian}
\caption{Lithuanian Data Set}
\label{Figure::lithuanian}
\end{figure}

\begin{figure}[]
\centering
\includegraphics[scale=0.6]{figs/dataset_Banana}
\caption{Banana (Moon) Data Set}
\label{Figure::banana}
\end{figure}

\begin{figure}[]
\centering
\includegraphics[scale=0.6]{figs/dataset_circle}
\caption{Circle Data Set}
\label{Figure::circle}
\end{figure}

Another purpose of this example is to show that we can reach the same level of detection by combining very simple and na√Øve model detectors, instead of spending time to find the proper model and to train that model on the data set, which is usually a time consuming task. In this example, hundreds of linear detectors are trained over on the data set. In order to have diverse detectors, we changed the ratio of data and their label during training.

As shown in Figure~\ref{Figure::lithuanian_all}, \ref{Figure::banana_all} and \ref{Figure::circle_all}, we have different trained detectors on these data sets. None of them, however, can properly classify the data sets. As we will show later, we can combine these detectors to construct a detector which can properly classify the data sets and its performance is better that each individual detector.

\begin{figure}[H]
\centering
\includegraphics[scale=0.6]{figs/AllClassifiersLithuanian}
\caption{Linear detectors trained on the Lithuanian Data Set}
\label{Figure::lithuanian_all}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[scale=0.6]{figs/AllClassifiersBanana}
\caption{Linear detectors trained on the Banana (Moon) Data Set}
\label{Figure::banana_all}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[scale=0.6]{figs/AllClassifiersCircle}
\caption{Linear detectors trained on the Circle Data Set}
\label{Figure::circle_all}
\end{figure}

By selecting the right models and combining them, we can increase the detection rate, as shown in Figures~\ref{fig:ROC_comparison_banana}, \ref{fig:ROC_comparison_lithuanian} and \ref{fig:ROC_comparison_circle}. The figures on the left show the original detectors and their ROC before combination. After selecting the proper detectors and combining them together (i.e. after just one iteration) we can reach the accuracy shown on the right figures, which show the combined detectors (in yellow) and their improved ROC curve. The next step is to show why combination provides better results and how we can obtain the same result just by combining parts of trained models.


\begin{figure}[H]
    \centering
    \begin{adjustbox}{minipage=\linewidth,scale=1.1}
    \begin{subfigure}[b]{0.5\columnwidth}
        \centering
        \includegraphics[width=\linewidth]{figs/ROCHLeft-Lithuanian}
        \caption{ROC before combination}
        \label{fig:ROC_left_lithuanian} 
    \end{subfigure}
    \begin{subfigure}[b]{0.5\columnwidth}
        \centering
        \includegraphics[width=\linewidth]{figs/ROCHRight-Lithuanian}
        \caption{ROC after combination}
        \label{fig:ROC_right_lithuanian}
    \end{subfigure}
    \caption{ROC comparison on Lithuanian data set before and after first round of combination}
    \label{fig:ROC_comparison_lithuanian}
    \end{adjustbox}
\end{figure}


\begin{figure}[H]
    \centering
    \begin{adjustbox}{minipage=\linewidth,scale=1.1}
    \begin{subfigure}[b]{0.5\columnwidth}
        \centering
        \includegraphics[width=\linewidth]{figs/ROCHLeft-Banana}
        \caption{ROC before combination}
        \label{fig:ROC_left_banana} 
    \end{subfigure}
    \begin{subfigure}[b]{0.5\columnwidth}
        \centering
        \includegraphics[width=\linewidth]{figs/ROCHRight-Banana}
        \caption{ROC after combination}
        \label{fig:ROC_right_banana}
    \end{subfigure}
    \caption{ROC comparison on Banana data set before and after first round of combination}
    \label{fig:ROC_comparison_banana}
    \end{adjustbox}
\end{figure}

\begin{figure}[H]
    \centering
    \begin{adjustbox}{minipage=\linewidth,scale=1.1}
    \begin{subfigure}[b]{0.5\columnwidth}
        \centering
        \includegraphics[width=\linewidth]{figs/ROCHLeft-Circle}
        \caption{ROC before combination}
        \label{fig:ROC_left_circle} 
    \end{subfigure}
    \begin{subfigure}[b]{0.5\columnwidth}
        \centering
        \includegraphics[width=\linewidth]{figs/ROCHRight-Circle}
        \caption{ROC after combination}
        \label{fig:ROC_right_circle}
    \end{subfigure}
    \caption{ROC comparison on Circle data set before and after first round of combination}
    \label{fig:ROC_comparison_circle}
    \end{adjustbox}
\end{figure}

For each data set, we show that there is no single detector that can classify the data set properly. 

\begin{figure}[t!] % "[t!]" placement specifier just for this example
\begin{subfigure}{0.48\textwidth}
\includegraphics[width=\linewidth]{figs/Lithuanian/20All-Classifiers}
\caption{First subfigure} % are you sure you want keep such captions ?!
\label{fig:Lithuanian_all_single_a}
\end{subfigure}\hspace*{\fill}
\begin{subfigure}{0.48\textwidth}
\includegraphics[width=\linewidth]{figs/Lithuanian/43All-Classifiers}
\caption{Second subfigure} \label{fig:Lithuanian_all_single_b}
\end{subfigure}

\medskip
\begin{subfigure}{0.48\textwidth}
\includegraphics[width=\linewidth]{figs/Lithuanian/44All-Classifiers}
\caption{Third subfigure} \label{fig:Lithuanian_all_single_c}
\end{subfigure}\hspace*{\fill}
\begin{subfigure}{0.48\textwidth}
\includegraphics[width=\linewidth]{figs/Lithuanian/65All-Classifiers}
\caption{Fourth subfigure} \label{fig:Lithuanian_all_single_d}
\end{subfigure}

\medskip
\begin{subfigure}{0.48\textwidth}
\includegraphics[width=\linewidth]{figs/Lithuanian/118All-Classifiers}
\caption{Fifth subfigure} \label{fig:Lithuanian_all_single_e}
\end{subfigure}\hspace*{\fill}
\begin{subfigure}{0.48\textwidth}
\includegraphics[width=\linewidth]{figs/Lithuanian/146All-Classifiers}
\caption{Sixth subfigure} \label{fig:Lithuanian_all_single_f}
\end{subfigure}

\caption{Some of best LDAs ( Linear Discriminant Analysis) that can be trained on the Lithuanian data set} \label{fig:Lithuanian_all_single}
\end{figure}

As it is shown in Figure~\ref{fig:Lithuanian_all_single}, with just one  Linear Discriminant Analysis (LDA), we cannot classify all the classes with 100\% accuracy. We can, however, see that it is possible to choose two of these detectors and combine them to obtain a better detector. As it is shown in Figure~\ref{fig:Dataset_ROC_Lithuanian}, we select detector from Figure~\ref{fig:Lithuanian_all_single_a}  and detectors from ~\ref{fig:Lithuanian_all_single_d} and combine them together.

\begin{figure}[H]
    \centering
    \begin{adjustbox}{minipage=\linewidth,scale=1.1}
    \begin{subfigure}[b]{0.5\columnwidth}
        \centering
        \includegraphics[width=\linewidth]{figs/Lithuanian/20Dataset-ROC}
        \caption{First selected LDA}
        \label{fig:Dataset_ROC_Lithuanian_a} 
    \end{subfigure}
    \begin{subfigure}[b]{0.5\columnwidth}
        \centering
        \includegraphics[width=\linewidth]{figs/Lithuanian/65Dataset-ROC}
        \caption{Second selected LDA}
        \label{fig:Dataset_ROC_Lithuanian_b}
    \end{subfigure}
    \caption{Selected LDA and their respective point compare to current ROC}
    \label{fig:Dataset_ROC_Lithuanian}
    \end{adjustbox}
\end{figure}

If we combine these two detectors with an $AND$ operation,  as shown in Figure~\ref{fig::combined_lithuanian_roc}, we can obtain a better result. 
The red point is the mapping of the combined detectors. % I do not understand this sentence, do you mean "The red point shows the performance of the combined detectors"?
As we can see if we re-calculate the ROC based on this new generated point we will have a greater AUC.

\begin{figure}[H]
\centering
\includegraphics[scale=0.6]{figs/Lithuanian/9999999-combined_All-Classifiers}
\caption{Selected detectors and the respective combined point on the ROC curve} % what is "the respective combined point"
\label{fig::combined_lithuanian_roc}
\end{figure}


A similar approach can be applied to the Banana data set. As you can see in Figure~\ref{fig:Banana_all_single}, like for the Lithuania data set, it is not possible to classify the data set completely with one detector.  In this specific example, we need at least three detectors in order to classify the data in an accurate way. 

\begin{figure}[t!] % "[t!]" placement specifier just for this example
\begin{subfigure}{0.48\textwidth}
\includegraphics[width=\linewidth]{figs/Banana/2All-Classifiers}
\caption{First subfigure} % It is better to update these captions. Or maybe just remove these caption, 'a', 'b', etc. are expressive enough 
\label{fig:Banana_all_single_a}
\end{subfigure}\hspace*{\fill}
\begin{subfigure}{0.48\textwidth}
\includegraphics[width=\linewidth]{figs/Banana/12All-Classifiers}
\caption{Second subfigure} \label{fig:Banana_all_single_b}
\end{subfigure}

\medskip
\begin{subfigure}{0.48\textwidth}
\includegraphics[width=\linewidth]{figs/Banana/22All-Classifiers}
\caption{Third subfigure} \label{fig:Banana_all_single_c}
\end{subfigure}\hspace*{\fill}
\begin{subfigure}{0.48\textwidth}
\includegraphics[width=\linewidth]{figs/Banana/54All-Classifiers}
\caption{Fourth subfigure} \label{fig:Banana_all_single_d}
\end{subfigure}

\medskip
\begin{subfigure}{0.48\textwidth}
\includegraphics[width=\linewidth]{figs/Banana/147All-Classifiers}
\caption{Fifth subfigure} \label{fig:Banana_all_single_e}
\end{subfigure}\hspace*{\fill}
\begin{subfigure}{0.48\textwidth}
\includegraphics[width=\linewidth]{figs/Banana/160All-Classifiers}
\caption{Sixth subfigure} \label{fig:Banana_all_single_f}
\end{subfigure}

\caption{Some of best LDA ( Linear Discriminant Analysis) that can be trained on Banana data set} \label{fig:Banana_all_single}
\end{figure}

Based on the observation in this example, it is better to select three detectors in order to classify the data set. If we select detector in Figure~\ref{fig:Banana_all_single_a}, Figure~\ref{fig:Banana_all_single_c} and Figure~\ref{fig:Banana_all_single_f}  and combine them together, we  expect to achieve a better detector than the other three detectors.  Figure~\ref{fig::combined_banana_roc} shows all these detectors in one plot.


\begin{figure}[H]
\centering
\includegraphics[scale=0.6]{figs/Banana/9999999-combined_selected_before_combination}
\caption{Best potential selected detectors on Banana data set}
\label{fig::combined_banana_roc}
\end{figure}

These examples show that it is not always necessary to train complex models, which takes time and resources, over the data set to obtain the best possible result. A similar result can be achieved by combining two or more simple models, which can be trained much faster. Also by increasing the number of these simple models, we can cover most of the cases (including the outliers) in the data set. Besides, even if we decide to train complex models, there is no guarantee that we can obtain the best results 
since every model has its own weaknesses. % this is not a good reason
That is to say some sort of combination is needed to help improve the result. %again this does not convince me.
In the next chapter, we show that how a set of meaningful models can be selected and combined to improves the results. We need such a strategy for selecting and combining to avoid training thousands of models and combining them all together. 
%the previous paragraph may need to be re-write.

The Circle data set is a good example where doing combination in an iterative way improves the accuracy, as shown in Figure~\ref{fig:Circle_all_single}. We could not find any two detectors that can be combined  to classify the whole data set. What we need here is to select all of these detectors and start  combining them two by two iteratively. In Figure~\ref{fig::combined_circle_roc}, we show the result of classifying iteratively  the whole data set with the help of a few simple detectors.


\begin{figure}[H] % "[t!]" placement specifier just for this example
\begin{subfigure}{0.48\textwidth}
\includegraphics[width=\linewidth]{figs/Circle/11-two-circle}
\caption{First subfigure} \label{fig:Circle_all_single_a}
\end{subfigure}\hspace*{\fill}
\begin{subfigure}{0.48\textwidth}
\includegraphics[width=\linewidth]{figs/Circle/22-two-circle}
\caption{Second subfigure} \label{fig:Circle_all_single_b}
\end{subfigure}

\medskip
\begin{subfigure}{0.48\textwidth}
\includegraphics[width=\linewidth]{figs/Circle/33-two-circle}
\caption{Third subfigure} \label{fig:Circle_all_single_c}
\end{subfigure}\hspace*{\fill}
\begin{subfigure}{0.48\textwidth}
\includegraphics[width=\linewidth]{figs/Circle/44-two-circle}
\caption{Fourth subfigure} \label{fig:Circle_all_single_d}
\end{subfigure}

\medskip
\begin{subfigure}{0.48\textwidth}
\includegraphics[width=\linewidth]{figs/Circle/55-two-circle}
\caption{Fifth subfigure} \label{fig:Circle_all_single_e}
\end{subfigure}\hspace*{\fill}
\begin{subfigure}{0.48\textwidth}
\includegraphics[width=\linewidth]{figs/Circle/66-two-circle}
\caption{Sixth subfigure} \label{fig:Circle_all_single_f}
\end{subfigure}

\caption{Series of 2 selected LDA ( Linear Discriminant Analysis) that can be trained and combined on the Circle data set} \label{fig:Circle_all_single}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[scale=0.6]{figs/Circle/9999999-combined_selected_before_combination}
\caption{Best potential selected detectors on the Circle data set}
\label{fig::combined_circle_roc}
\end{figure}




