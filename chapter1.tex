\chapter{Introduction} \label{chapter1}

%\section{Introduction}

Intrusion Detection Systems (IDSs) have become important tools that help security administrators to identify and analyze unauthorized computer or network activities, from both outsider and insider attacks.
The dominant detection methodologies are signature-based and anomaly-based approaches.
Signature-based (or misuse) IDSs look for events that correspond to patterns of known attacks.
They can provide a high level of accuracy, however, are limited to detecting known attacks and vulnerable to polymorphic attacks (which are capable of changing their signatures as they propagate).

Anomaly detection systems (ADSs), on the other hand, monitor for significant deviations from normal system behavior.
They are typically trained (using machine learning and data mining techniques) on data sets collected over a period of attack-free activities to learn the normal system behavior, and then deployed to detect deviations from the expected system behavior.
These deviations are reported as anomalous events, although they are not necessarily malicious activities or attacks because they may also correspond to coding or configuration errors.
An ADS can detect novel attacks (not known during training time) but generate large number of false alarms, because it is difficult to obtain complete descriptions of complex system behavior, which may change over time.

IDSs can also be categorized depending on their location of deployment into network- and host-based detection systems (a more comprehensive taxonomy of IDSs can be found in \cite{Liao2013}).
Network-based IDSs monitor and analyze traffic to and from all devices on the network.
They may be located anywhere in the network; integrated in network devices (e.g., switches and routers) or as a standalone device.
A host-based IDS runs on a host computer and monitors sensitive activities on this host, such as unauthorized access, modification of files, or system calls.

In this work, we focus on host-based anomaly detection using system calls -- the gateway between user and kernel mode.
Short sequences of system calls have been shown consistent with normal host operation, and can be used to detect attacks \cite{Forrest1996,Warrender1999}.
A large number of research studies have investigated different machine learning and data mining techniques for detecting anomalies in system call sequences \cite{Forrest2008, Creech2014, Yolacan2014, Warrender1999}.

One of the early work in the field was done by Warrender and Forrest~\cite{Warrender1999}. They introduced the technique called Sequence Time-Delay Embedding (STIDE)  "acquires a model of normal behavior by segmenting training data into fixed-length sequence"~\cite{Warrender1999}. This is done by sliding a detector window of length $DW$ (from now on $DW$) over the training data. Each of this sequence obtained from the data stream is stored in a "normal database" of sequences of length $DW$. A similarity metrics are then used to establish the degree of similarity between the test data and the model of normal behaviour that is obtained in the previous step. Sequence of length DW is obtained from the test data using a sliding window, and for each length DW sequence the similarity metric simply established whether that sequence exists or does not exist in the normal database~\cite{Tan2002}.
STIDE is a good starting point, but still has a number of issues. First; what is the best anomaly detection window size? Tan et al~\cite{Tan2002} have shown that for New Mexico data set the minimum window size should be 6, otherwise we are going to miss detection of some intrusions. Even in their work they mentioned they cannot specify a particular window size that will work for all data set, therefore we should find minimum window size based on our data in the training phase.
The other issue with STIDE or other detector that want to model the normal behaviour of the system is that learning phase is too much time consuming, you should run the system in a very clean environment for a long time and store traces of the normal behaviour and yet you are not completely sure that you can model the normal behaviour of the system based on the traces you had stored so far, besides that the normal behaviour of the system will change in the course of time.

The study by Eskin et al ~\cite{Creech2014} shows, that it is possible estimate the proper window size for each data set instead of having constant one. And by selecting and working on dynamic window size they get improvement in result. They estimating the optimal window size by two different techniques. Entropy modeling, method which determines the optimal single window size, and probability modeling method that takes into account context dependent window size. In their work, they used a new method for modeling system call traces using sparse
Markov transducers. By using sparse
Markov transducers they were be able to estimates the best window size depending on the
specific system calls in the subsequence based on their performance over the training data~\cite{Creech2014}.


One of the recent study by Creech et al~\cite{Creech2014} is a good case that shows, using machine learning techniques on system call traces to find anomalies is still a hot topic. They proposed Extreme Learning Machine (ELM), which is a new and strong technique in the area of neural network family. The advantage of ELM comparison to previous techniques is that, it is much faster and more scalable. Beside it is a very lightweight component and it is still very accurate which makes it very interesting. However, it has its own limit and we cannot improve ELM result which we will address the solution in my thesis.


Among these, techniques based on Hidden Markov Models (HMMs)~-- probabilistic models for sequential data~-- have been shown to produce a high level of detection accuracy \cite{Du2004,Gao2002,Hoang2004,Hu2010,Wang2004,Warrender1999,Zhang2003,Khreich2009-ICC, Sultana2012, Murtaza2012}.

The success of an ADS depends largely on the model of normal behavior.
A single HMM may not, however, provide adequate approximation of the underlying data distribution of a complex host system behavior, due to the many local maxima of the likelihood function \cite{Khreich2009-ICC}.
Ensemble methods have been used to improve the overall system accuracy by combining the outputs of several \textit{accurate and diverse} models \cite{Kittler1998,Dietterich2000,Kuncheva2004b,Zhou2012}.
In particular, combining the outputs from multiple HMMs, each trained with a different number of states, in the Receiver Operating Characteristics (ROC) space according to the Iterative Boolean Combination (IBC), has been shown to provide a significant improvement in the detection accuracy of system call anomalies \cite{Khreich2010-ICPR}.

The IBC is a general decision-level combination technique that attempts to select the decision thresholds (from each input detector) and the Boolean functions that maximize the overall ROC convex hull of the combined ensemble \cite{Khreich2010-ICPR}.
As presented in Algorithm~\ref{IBC1}, given $K$ soft detectors that assign scores or probabilities to the input samples, (which can be converted to a crisp detector by setting a threshold on the scores as further detailed in section~\ref{sec:boolean-combination}) , the IBC algorithm starts by combining the outputs of the first two detectors (in the order of input), and then proceeds \textit{sequentially} by combining the resulting combinations with the outputs of third detector, and so on, until the $K^{th}$ detector is combined.
It can then re-iterate to combine the resulting combinations with the original detectors.

The IBC technique provides a practical way for combining relatively large number of detectors while avoiding the exponential explosion of Boolean combinations.
As detailed in Section~\ref{sub:complexity}, applying all Boolean functions using an exhaustive brute-force search to determine optimal combinations leads to an exponential number of combinations, which is prohibitive even for a small number of detectors \cite{Barreno2008}.
Even the \textit{pairwise} Bruteforce Boolean Combination (BBC2), which is used as a baseline reference in our experiments, requires an exponential number of combinations, which is equal to the square of the number of detectors (see Algorithm~\ref{PBC} without the pruning mechanisms).

However, the sequential combinations of soft detectors according to IBC still faces challenges, when a large number of detectors ($K$) is presented for combinations.
First, it produces a sequence of combination rules that grows linearly with $K$ (and the number of iterations), which becomes difficult to analyse and understand.
Furthermore, it makes the algorithm sensitive to the order in which the detectors are input for combinations, which increases the effort required to find best subset for operations.
In any case, combining all available detectors without any pruning mechanism may be inefficient due to the redundancy in their outputs.

In this work, we propose an ADS based on a Pruned Boolean Combination (PBC) algorithm, which employs two novel pruning techniques to select a subset of diverse and accurate detectors for combination, while discarding the remaining ones.
Although both pruning techniques are based on Cohen's Kappa \cite{Cohen1995a} measure, they differ in the way they compute the diversity and accuracy of the selected detectors for combinations.
While the first technique relies only on Kappa measure (MinMax-Kappa), the second uses both Kappa and the ROC convex hull (ROCCH-Kappa).
Therefore, our proposed PBC approach provides an efficient way to prune and combine large number of detectors, which avoids the exponential explosion of combinations of brute-force techniques, and reduces the sequence of combinations provided by IBC.

We evaluate our PBC-based ADS using ADFA Linux Data set (ADFA-LD)\footnote{http://www.cybersecurity.unsw.adfa.edu.au/ADFA IDS Datasets/}, which has been recently made publicly available on the website of the University of New South Wales \cite{Creech2013a}.
The performance of the PBC using both pruning techniques are compared to that of IBC and BBC2 in terms of ROC analysis and time complexity.
The results show that PBC with both pruning techniques are capable of maintaining similar overall accuracy as measured by the ROC curves to that of IBC and BBC2.
However, the time required for searching and selecting (or pruning) the subset of detectors from the same ensemble of detectors is on average three magnitudes lower for PBC with MinMax-Kappa pruning than that of BBC2.
Furthermore, the experimental results show that PBC with both pruning techniques always provides two crisp detectors for combination (during the operations), while IBC provided an average of $11$ detectors to achieve the same operating point.


\section{System Calls}
\label{sec:system-calls}
A system call is a request in a Unix-like operating system made via a software interrupt by an active process for a service performed by the kernel.
Users' requests are made from higher level applications  to the kernel via a platform-dependent set of system calls. System calls provide an essential interface between a process and the operating system. Applications must invoke system calls to request kernel services, such as access to standard input/output, physical devices and network resources. Every system call has a unique number (known by the kernel) and a set of arguments. Some example of system calls used for file managements are as follows: open(), read(), write(), and close(). System call traces have different lengths that depends on the complexity and execution time of the process.
Traces of a wide variety of attacks, including buffer overflows, symbolic link, decode and SYN floods may appear at the system call level and differ from normal behavior of privileged processes, (\cite{Forrest1996}; \cite{Hofmeyr1998}; \cite{Kosoresow1997}; \cite{Somaayaji2002}; \cite{Warrender1999}). Furthermore, after gaining root privileges on a host, attackers will typically try to maintain administrative access on the compromised host (\cite{Mitnick2005}), by for instance installing backdoors and rootkits. Attackers will also attempt to distribute Trojan horses, using for instance comprised email addresses or shared network folders, to compromise other systems. These malicious activities may also invoke different sequences of system calls than those generated during normal execution of a process.

\section{HMM-based Anomaly Detection using System Call Sequences}
\label{sec:ads-hmm}


The temporal order of system calls is used to represent the normal behavior of a privileged process, while an unusual burst will occur during an attack \cite{Forrest1996,Warrender1999}.
In \cite{Forrest1996,Warrender1999}, the authors proposed a host-based ADS using a simple sequence matching technique for detecting anomalous system call sequences, generated from UNIX privileged processes \cite{Forrest1996}.
System call traces provided for training are first segmented into fixed-length \textit{contiguous} sequences, using a fixed-size sliding window, shifted by one symbol to build the normal profile.
These sequences are then stored in a database that represents the normal process behavior.
During operations, a sliding window (having the same size used to construct the normal profile) to scan the system calls generated by the monitored process for anomalies -- sequences that are not found in the normal database.

Several statistical and machine learning techniques have been investigated over the last two decades for detecting system call anomalies \cite{Forrest2008}.
Application of machine learning techniques include neural networks \cite{Ghosh1999}, k-nearest neighbors \cite{Liao2002}, Markov models \cite{Jha2001,Marceau2000}, Bayesian models \cite{Kruegel2003a}.
Among these, techniques based on discrete HMMs have been shown to produce a high level of detection accuracy \cite{Warrender1999,Gao2002,Gao2003,Zhang2003,Du2004,Wang2004,Hoang2004,Khreich2009-ICC,Chen2009,Hu2010,Wang2010}.

A discrete HMM is a stochastic process for sequential data \cite{Khreich2012-INS,Rabiner1989}.
An HMM is determined by two interrelated mechanisms -- a latent Markov chain having a finite number of states $N$, and a set of observation probability distributions, each one associated with a state.
 Starting from an initial state $S_{i}\in\{S_{1},...,S_{N}\}$, determined by the initial state probability distribution $\pi_{i}$, at each discrete-time instant, the process transits from state $S_{i}$ to state $S_{j}$ according to the transition probability distribution $a_{ij}$.
The process then emits a symbol $v_{k}$, from a finite alphabet $V=\{v_{1},\ldots,v_{M}\}$ of size $M$ symbols, according to the discrete-output probability distribution $b_{j}(v_{k})$ of the current state $S_{j}$.
HMM is commonly parametrized by $\lambda=(\pi,A,B)$, where the vector $\pi=\{\pi_{i}\}$ is the initial state probability distribution, matrix $A=\{a_{ij}\}$ is the state transition probability distribution, and matrix $B=\{b_{j}(v_{k})\}$ is the state output probability distribution, ($1\leq i,j\leq N$ and $1\leq k\leq M$).
% However, the time and memory complexity for training an HMM with $N$ states according to Baum-Welch algorithm is $\mathcal{O}(N^2T)$ and $\mathcal{O}(NT)$ respectively, for a sequence of length $T$ symbols.

A well trained HMM provides a compact detector that captures the underlying structure of a process based on the temporal order of system calls, and detects deviations from normal system call sequences with high accuracy and tolerance to noise.
Training an HMM from a sequence (or a block) of observation symbols, $o_{1:T}$, aims at estimating HMM parameters $\lambda$ to best fit the training data.
 Typically, parameters estimation consists of maximizing the likelihood of the training data over HMM parameters space, $P(o_{1:T}\mid\lambda)$.
Since this likelihood depends on the latent states, there is no known analytical solution to the learning problem.
Iterative optimization techniques, such as the Baum-Welch (BW) algorithm \cite{Baum1970}, are applied to estimate the HMM parameters over several training iterations, until the likelihood function is maximized.
During operation, the likelihood of a new observation sequence $o_{1:T}$ given a trained HMM $\lambda$, $P(o_{1:T}\mid\lambda)$ is typically evaluated by using the Forward-Backward (FB) algorithm \cite{Khreich2012-INS,Rabiner1989}.
Setting a threshold on the output probabilities of HMM, provides a decision whether the sequence is normal or anomalous.

Some researchers investigated the effect of the number of states on the performance of HMM detectors, and found that $N$ value may have a great impact on the overall performance \cite{Yeung2003,Khreich2009-ICC}.
In particular, combining the outputs from multiple HMMs, each trained with a different $N$ value, in the ROC space according to the IBC technique (as described in the next section), has been shown to provide a significant improvement in the detection accuracy of system call anomalies \cite{Khreich2010-ICPR}.
 
